{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9f49db-9f40-4176-9d8b-79eed53bfa8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build with AI United 2024 Hands-on - Multimodal Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e94777-efe9-4a3a-a6d9-eeb4c5991c48",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Retrieval augmented generation (RAG)는 LLM이 외부 data에 접근할 수 있게 함으로써 hallucinations(환각)을 완화하기 위한 매커니즘이며, 이미 널리 사용되는 패러다임이 되었습니다.\n",
    "\n",
    "이번 핸즈온에서는 텍스트와 이미지로 채워진 재무 문서에 대해 Q&A를 수행하는 multimodal RAG를 수행하는 방법에 대해 다룹니다.\n",
    "\n",
    "*#vertex_ai #gemini #vertex_ai_gemini_api #text_embedding #multimodal_embedding #document_search_engine #RAG*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8164c45-a970-4330-a183-bd9ac7458bcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Objectives\n",
    "\n",
    "이 notebook은 멀티모달 검색 증강 생성(RAG)을 사용해 문서 검색 엔진을 구축하는 방법을 단계별로 안내합니다:\n",
    "\n",
    "1. 텍스트와 이미지가 모두 포함된 문서의 메타데이터 추출 및 저장, 문서 임베딩 생성하기\n",
    "2. 텍스트 쿼리로 메타데이터를 검색하여 유사한 텍스트나 이미지를 찾습니다.\n",
    "3. 이미지 쿼리로 메타데이터를 검색하여 유사한 이미지 찾기\n",
    "4. 텍스트 쿼리를 입력으로 사용하여 텍스트와 이미지를 모두 사용하여 문맥에 맞는 답을 검색하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8c5ed-ae11-46e0-b17b-c5ed74509ae9",
   "metadata": {},
   "source": [
    "### Costs\n",
    "\n",
    "이 튜토리얼은 Google Cloud의 Vertex AI를 사용하기에 비용이 청구됩니다.\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
    "\n",
    "Build with AI United 2024 핸즈온에 참여하시는 경우, credit이 제공되오니 반드시 등록 후 사용하시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c112d-f94a-4cd2-b85d-83f1d3e84ba5",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "이 notebook은 [Lavi Nigam](https://github.com/lavinigam-gcp)의 [Multimodal Retrieval Augmented Generation (RAG) using Vertex AI Gemini API](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb)를 참고하여 제작되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed100e-0ee0-44b4-b5dd-f8a943a67a51",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab31b2f-2b8e-4bfc-a3e2-dbfc6972929b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Vertex AI SDK for Python and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b60a322-29df-4c23-81f5-04b0cba04b17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.48.0)\n",
      "Requirement already satisfied: pymupdf in ./.local/lib/python3.10/site-packages (1.24.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.29.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.19.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.21.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.15)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.1 in ./.local/lib/python3.10/site-packages (from pymupdf) (1.24.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.11.0)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from grpcio<2.0dev,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5f19d-6e93-4309-851d-aa06bb3718e0",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af55713-cbed-4deb-a94f-8f2bcae37470",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563bc20-e35e-4340-9cbf-31566f56ecf5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb254ae4-0d4e-49bc-ae25-85281adda825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your project ID is: bwai-handson-lab\n"
     ]
    }
   ],
   "source": [
    "# Define project information\n",
    "\n",
    "import sys\n",
    "\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "#LOCATION = \"asia-northeast3\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "# if not running on colab, try to get the PROJECT_ID automatically\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee269e5-84fb-45a8-af50-4c8de6fb04da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5514a-7fb5-4355-8839-995c5344495d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfa54f8-9ef0-4f38-a65f-131661851caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4099f77-49c1-48fe-a53a-f57852933a3b",
   "metadata": {},
   "source": [
    "### Load the Gemini 1.0 Pro and Gemini 1.0 Pro Vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b24ce1d-5c6b-4b43-8f72-71060d7ffd52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58835877-9b37-4daa-842e-0ef69a3ed8ee",
   "metadata": {},
   "source": [
    "### Get documents and images from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e17a656e-ebf2-44a5-82f4-ac138e69821f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying gs://bwai-handson-bucket/documents2/google-10k-sample-kr-part2.pdf...\n",
      "Copying gs://bwai-handson-bucket/documents/google-10k-sample-part1.pdf...       \n",
      "Copying gs://bwai-handson-bucket/documents/google-10k-sample-part2.pdf...       \n",
      "Copying gs://bwai-handson-bucket/documents2/google-10k-sample-kr-part1.pdf...   \n",
      "/ [4/4 files][  1.8 MiB/  1.8 MiB] 100% Done                                    \n",
      "Operation completed over 4 objects/1.8 MiB.                                      \n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "# download documents and images used in this notebook\n",
    "!gsutil -m rsync -r gs://bwai-handson-bucket .\n",
    "print(\"Download completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abfd94f-2056-4b0e-86f4-e603052325c3",
   "metadata": {},
   "source": [
    "## Building metadata of documents containing text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0909c03-b60e-48d9-94c8-3d610ab33e5b",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "핸즈온에서 사용할 원본 데이터는 회사의 재무 성과, 경영, 관리 및 위험 요소에 대한 포괄적인 개요를 제공하는 Google-10K의 수정된 버전입니다. 원본 문서가 다소 방대하므로 대신 14페이지만 있는 수정된 버전을 사용하게 됩니다. 축소되었지만 샘플 문서에는 여전히 표, 차트, 그래프 등의 이미지와 함께 텍스트가 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a613c9-bccf-44bd-adbf-27ef7f759c41",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract and store metadata of text and images from a document\n",
    "\n",
    "멀티모달 RAG 시스템을 구축하기 전에 문서에 있는 모든 텍스트와 이미지의 metadata를 확보하는 것이 중요합니다. 참조 및 인용을 위해 metadata에는 페이지 번호, 파일 이름, 이미지 개수 등의 필수 요소가 포함되어야 합니다. 따라서 다음 단계로 데이터를 쿼리할 때 유사성 검색(similarity search)을 수행하는 데 필요한 metadata에서 임베딩을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510f4470-df76-40db-8397-921b95ab75de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "from IPython.display import display\n",
    "import PIL\n",
    "import fitz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Image,\n",
    ")\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import Image as vision_model_Image\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754c8c3-8ce4-4614-ac84-0cf742e05b50",
   "metadata": {},
   "source": [
    "`get_gemini_response()` 함수는 주어진 입력에 대해 multimodal의 추론(inference)를 수행한 후 생성된 결과를 하나의 문자열로 결합하여 최종 결과를 return합니다.\n",
    "\n",
    "이번 핸즈온에서는 multimodal 모델로 `gemini 1.0 pro vision`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf55809-6aab-4c45-a043-f1d57edb0c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gemini_response(\n",
    "    generative_multimodal_model,\n",
    "    model_input: List[str],\n",
    "    stream: bool = True,\n",
    "    generation_config: Optional[GenerationConfig] = GenerationConfig(\n",
    "        temperature=0.2, max_output_tokens=2048\n",
    "    ),\n",
    "    safety_settings: Optional[dict] = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    ") -> str:\n",
    "\n",
    "    response = generative_multimodal_model.generate_content(\n",
    "        model_input,\n",
    "        generation_config=generation_config,\n",
    "        stream=stream,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    response_list = []\n",
    "\n",
    "    for chunk in response:\n",
    "        try:\n",
    "            response_list.append(chunk.text)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Exception occurred while calling gemini. Something is wrong. Lower the safety thresholds [safety_settings: BLOCK_NONE ] if not already done. -----\",\n",
    "                e,\n",
    "            )\n",
    "            response_list.append(\"Exception occurred\")\n",
    "            continue\n",
    "    response = \"\".join(response_list)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7310e-b6af-4877-97c5-d61338132614",
   "metadata": {},
   "source": [
    "텍스트와 이미지의 임베딩을 위해 아래의 임베딩 모델을 사용합니다.\n",
    "\n",
    "- text embedding model: [textembedding-gecko@latest](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings?hl=ko)\n",
    "- multimodal embedding model: [multimodalembedding@001](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-embeddings?hl=ko)\n",
    "\n",
    "아래 코드에서는 텍스트와 이미지 데이터를 고차원의 embedding vecter로 변환하는 함수를 포함합니다.\n",
    "\n",
    "- `get_text_embedding_from_text_embedding_model()`: 텍스트 문자열을 입력받아 text_embedding_model을 이용하여 텍스트 임베딩 생성\n",
    "- `get_image_embedding_from_multimodal_embedding_model()`: 이미지 경로 및 텍스트(선택)를 입력받아 multimodal_embedding_model을 이용하여 이미지 임베딩 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46bf79e7-c846-4bbb-b1e4-7d60ba08ded5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@latest\")\n",
    "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
    "    \"multimodalembedding@001\"\n",
    ")\n",
    "\n",
    "# Functions for getting text and image embeddings\n",
    "\n",
    "def get_text_embedding_from_text_embedding_model(\n",
    "    text: str,\n",
    "    return_array: Optional[bool] = False,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generates a numerical text embedding from a provided text input using a text embedding model.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string to be embedded.\n",
    "        return_array: If True, returns the embedding as a NumPy array.\n",
    "                      If False, returns the embedding as a list. (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        list or numpy.ndarray: A 768-dimensional vector representation of the input text.\n",
    "                               The format (list or NumPy array) depends on the\n",
    "                               value of the 'return_array' parameter.\n",
    "    \"\"\"\n",
    "    embeddings = text_embedding_model.get_embeddings([text])\n",
    "    text_embedding = [embedding.values for embedding in embeddings][0]\n",
    "\n",
    "    if return_array:\n",
    "        text_embedding = np.fromiter(text_embedding, dtype=float)\n",
    "\n",
    "    # returns 768 dimensional array\n",
    "    return text_embedding\n",
    "\n",
    "\n",
    "def get_image_embedding_from_multimodal_embedding_model(\n",
    "    image_uri: str,\n",
    "    embedding_size: int = 512,\n",
    "    text: Optional[str] = None,\n",
    "    return_array: Optional[bool] = False,\n",
    ") -> list:\n",
    "    \"\"\"Extracts an image embedding from a multimodal embedding model.\n",
    "    The function can optionally utilize contextual text to refine the embedding.\n",
    "\n",
    "    Args:\n",
    "        image_uri (str): The URI (Uniform Resource Identifier) of the image to process.\n",
    "        text (Optional[str]): Optional contextual text to guide the embedding generation. Defaults to \"\".\n",
    "        embedding_size (int): The desired dimensionality of the output embedding. Defaults to 512.\n",
    "        return_array (Optional[bool]): If True, returns the embedding as a NumPy array.\n",
    "        Otherwise, returns a list. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the image embedding values. If `return_array` is True, returns a NumPy array instead.\n",
    "    \"\"\"\n",
    "    # image = Image.load_from_file(image_uri)\n",
    "    image = vision_model_Image.load_from_file(image_uri)\n",
    "    embeddings = multimodal_embedding_model.get_embeddings(\n",
    "        image=image, contextual_text=text, dimension=embedding_size\n",
    "    )  # 128, 256, 512, 1408\n",
    "    image_embedding = embeddings.image_embedding\n",
    "\n",
    "    if return_array:\n",
    "        image_embedding = np.fromiter(image_embedding, dtype=float)\n",
    "\n",
    "    return image_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034b366-7cb3-416a-80a4-a16ffe2d4bb8",
   "metadata": {},
   "source": [
    "아래 코드는 RAG를 위해 사용될 PDF에서 텍스트와 이미지를 추출합니다. 텍스트 문서는 지정된 크기의 chunk로 나누고, 각 chunk에 대한 임베딩을 생성합니다.\n",
    "\n",
    "- `get_pdf_doc_object()`: RAG를 위한 document로 주어지는 pdf에서 pdf 내용과 page 수를 추출\n",
    "- `get_text_overlapping_chunk()`: 텍스트 문서를 지정된 크기의 청크로 나누고 청크 사이를 겹쳐서 문맥을 보존\n",
    "- `get_page_text_embedding()`: embedding model을 사용하여 각 텍스트 청크에 대한 임베딩 생성\n",
    "- `get_chunk_text_metadata()`: 지정된 페이지 객체에서 텍스트를 추출하여 청크로 나누고 각 청크에 대한 임베딩을 생성\n",
    "- `get_image_for_gemini()`: PDF 문서에서 이미지를 추출하여 JPEG 형식으로 변환하고 지정된 디렉터리에 저장한 다음 PIL Image Object로 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "516d58df-f7a1-445e-85a1-b3991ce6997c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pdf_doc_object(pdf_path: str) -> tuple[fitz.Document, int]:\n",
    "    \n",
    "    # Open the PDF file\n",
    "    doc: fitz.Document = fitz.open(pdf_path)\n",
    "\n",
    "    # Get the number of pages in the PDF file\n",
    "    num_pages: int = len(doc)\n",
    "\n",
    "    return doc, num_pages\n",
    "\n",
    "\n",
    "def get_text_overlapping_chunk(\n",
    "    text: str, character_limit: int = 1000, overlap: int = 100\n",
    ") -> dict:\n",
    "\n",
    "    if overlap > character_limit:\n",
    "        raise ValueError(\"Overlap cannot be larger than character limit.\")\n",
    "\n",
    "    # Initialize variables\n",
    "    chunk_number = 1\n",
    "    chunked_text_dict = {}\n",
    "\n",
    "    # Iterate over text with the given limit and overlap\n",
    "    for i in range(0, len(text), character_limit - overlap):\n",
    "        end_index = min(i + character_limit, len(text))\n",
    "        chunk = text[i:end_index]\n",
    "\n",
    "        # Encode and decode for consistent encoding\n",
    "        chunked_text_dict[chunk_number] = chunk.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        # chunked_text_dict[chunk_number] = chunk.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")  # for korean\n",
    "\n",
    "        # Increment chunk number\n",
    "        chunk_number += 1\n",
    "\n",
    "    return chunked_text_dict\n",
    "\n",
    "\n",
    "def get_page_text_embedding(text_data: Union[dict, str]) -> dict:\n",
    "\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    if isinstance(text_data, dict):\n",
    "        # Process each chunk\n",
    "        # print(text_data)\n",
    "        for chunk_number, chunk_value in text_data.items():\n",
    "            text_embd = get_text_embedding_from_text_embedding_model(text=chunk_value)\n",
    "            embeddings_dict[chunk_number] = text_embd\n",
    "    else:\n",
    "        # Process the first 1000 characters of the page text\n",
    "        text_embd = get_text_embedding_from_text_embedding_model(text=text_data)\n",
    "        embeddings_dict[\"text_embedding\"] = text_embd\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def get_chunk_text_metadata(\n",
    "    page: fitz.Page,\n",
    "    character_limit: int = 1000,\n",
    "    overlap: int = 100,\n",
    "    embedding_size: int = 128,\n",
    ") -> tuple[str, dict, dict, dict]:\n",
    "\n",
    "    if overlap > character_limit:\n",
    "        raise ValueError(\"Overlap cannot be larger than character limit.\")\n",
    "\n",
    "    # Extract text from the page\n",
    "    text: str = page.get_text().encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    # text: str = page.get_text().encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")  # for korean\n",
    "\n",
    "    # Get whole-page text embeddings\n",
    "    page_text_embeddings_dict: dict = get_page_text_embedding(text)\n",
    "\n",
    "    # Chunk the text with the given limit and overlap\n",
    "    chunked_text_dict: dict = get_text_overlapping_chunk(text, character_limit, overlap)\n",
    "    # print(chunked_text_dict)\n",
    "\n",
    "    # Get embeddings for the chunks\n",
    "    chunk_embeddings_dict: dict = get_page_text_embedding(chunked_text_dict)\n",
    "    # print(chunk_embeddings_dict)\n",
    "\n",
    "    # Return all extracted data\n",
    "    return text, page_text_embeddings_dict, chunked_text_dict, chunk_embeddings_dict\n",
    "\n",
    "\n",
    "def get_image_for_gemini(\n",
    "    doc: fitz.Document,\n",
    "    image: tuple,\n",
    "    image_no: int,\n",
    "    image_save_dir: str,\n",
    "    file_name: str,\n",
    "    page_num: int,\n",
    ") -> Tuple[Image, str]:\n",
    "\n",
    "    # Extract the image from the document\n",
    "    xref = image[0]\n",
    "    pix = fitz.Pixmap(doc, xref)\n",
    "\n",
    "    # Convert the image to JPEG format\n",
    "    pix.tobytes(\"jpeg\")\n",
    "\n",
    "    # Create the image file name\n",
    "    image_name = f\"{image_save_dir}/{file_name}_image_{page_num}_{image_no}_{xref}.jpeg\"\n",
    "\n",
    "    # Create the image save directory if it doesn't exist\n",
    "    os.makedirs(image_save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the image to the specified location\n",
    "    pix.save(image_name)\n",
    "\n",
    "    # Load the saved image as a Gemini Image Object\n",
    "    image_for_gemini = Image.load_from_file(image_name)\n",
    "\n",
    "    return image_for_gemini, image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c5473-4c32-475d-8f12-6fdd8faea403",
   "metadata": {},
   "source": [
    "`get_document_metadata()` 함수는 문서에서 텍스트와 이미지 metadata를 추출하여 텍스트 metadata와 이미지 metadata라는 두 개의 DataFrame을 출력으로 반환합니다. 텍스트 metadata와 이미지 metadata를 모두 추출하여 저장하는 이유는 둘 중 하나만 사용하는 것만으로는 적절한 답변이 나오지 않기 때문입니다. 예를 들어, 관련 답변이 문서 내에 시각적 형태로 존재할 수 있지만 텍스트 기반 RAG는 시각적 이미지를 고려할 수 없습니다.\n",
    "\n",
    "- text metadata: PDF의 각 page로부터 추출된 page text, chunked text dictionaries, chunk embedding dictionaries를 포함\n",
    "- image metadata: PDF의 각 image로부터 추출된 image path, image description, image embeddings (with and without context), image description text embedding을 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d127e76-0f23-4059-bf66-b0f61e479b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_metadata_df(\n",
    "    filename: str, text_metadata: Dict[Union[int, str], Dict]\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    final_data_text: List[Dict] = []\n",
    "\n",
    "    for key, values in text_metadata.items():\n",
    "        for chunk_number, chunk_text in values[\"chunked_text_dict\"].items():\n",
    "            data: Dict = {}\n",
    "            data[\"file_name\"] = filename\n",
    "            data[\"page_num\"] = int(key) + 1\n",
    "            data[\"text\"] = values[\"text\"]\n",
    "            data[\"text_embedding_page\"] = values[\"page_text_embeddings\"][\n",
    "                \"text_embedding\"\n",
    "            ]\n",
    "            data[\"chunk_number\"] = chunk_number\n",
    "            data[\"chunk_text\"] = chunk_text\n",
    "            data[\"text_embedding_chunk\"] = values[\"chunk_embeddings_dict\"][chunk_number]\n",
    "\n",
    "            final_data_text.append(data)\n",
    "\n",
    "    return_df = pd.DataFrame(final_data_text)\n",
    "    return_df = return_df.reset_index(drop=True)\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def get_image_metadata_df(\n",
    "    filename: str, image_metadata: Dict[Union[int, str], Dict]\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    final_data_image: List[Dict] = []\n",
    "    for key, values in image_metadata.items():\n",
    "        for _, image_values in values.items():\n",
    "            data: Dict = {}\n",
    "            data[\"file_name\"] = filename\n",
    "            data[\"page_num\"] = int(key) + 1\n",
    "            data[\"img_num\"] = int(image_values[\"img_num\"])\n",
    "            data[\"img_path\"] = image_values[\"img_path\"]\n",
    "            data[\"img_desc\"] = image_values[\"img_desc\"]\n",
    "            # data[\"mm_embedding_from_text_desc_and_img\"] = image_values[\n",
    "            #     \"mm_embedding_from_text_desc_and_img\"\n",
    "            # ]\n",
    "            data[\"mm_embedding_from_img_only\"] = image_values[\n",
    "                \"mm_embedding_from_img_only\"\n",
    "            ]\n",
    "            data[\"text_embedding_from_image_description\"] = image_values[\n",
    "                \"text_embedding_from_image_description\"\n",
    "            ]\n",
    "            final_data_image.append(data)\n",
    "\n",
    "    return_df = pd.DataFrame(final_data_image).dropna()\n",
    "    return_df = return_df.reset_index(drop=True)\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def get_document_metadata(\n",
    "    generative_multimodal_model,\n",
    "    pdf_folder_path: str,\n",
    "    image_save_dir: str,\n",
    "    image_description_prompt: str,\n",
    "    embedding_size: int = 128,\n",
    "    generation_config: Optional[GenerationConfig] = GenerationConfig(\n",
    "        temperature=0.2, max_output_tokens=2048\n",
    "    ),\n",
    "    safety_settings: Optional[dict] = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "    add_sleep_after_page: bool = False,\n",
    "    sleep_time_after_page: int = 2,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    text_metadata_df_final, image_metadata_df_final = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    for pdf_path in glob.glob(pdf_folder_path + \"/*.pdf\"):\n",
    "        print(\n",
    "            \"\\n\\n\",\n",
    "            \"Processing the file: ---------------------------------\",\n",
    "            pdf_path,\n",
    "            \"\\n\\n\",\n",
    "        )\n",
    "\n",
    "        doc, num_pages = get_pdf_doc_object(pdf_path)\n",
    "\n",
    "        file_name = pdf_path.split(\"/\")[-1]\n",
    "\n",
    "        text_metadata: Dict[Union[int, str], Dict] = {}\n",
    "        image_metadata: Dict[Union[int, str], Dict] = {}\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            print(f\"Processing page: {page_num + 1}\")\n",
    "\n",
    "            page = doc[page_num]\n",
    "\n",
    "            text = page.get_text()\n",
    "            (\n",
    "                text,\n",
    "                page_text_embeddings_dict,\n",
    "                chunked_text_dict,\n",
    "                chunk_embeddings_dict,\n",
    "            ) = get_chunk_text_metadata(page, embedding_size=embedding_size)\n",
    "\n",
    "            text_metadata[page_num] = {\n",
    "                \"text\": text,\n",
    "                \"page_text_embeddings\": page_text_embeddings_dict,\n",
    "                \"chunked_text_dict\": chunked_text_dict,\n",
    "                \"chunk_embeddings_dict\": chunk_embeddings_dict,\n",
    "            }\n",
    "\n",
    "            images = page.get_images()\n",
    "            image_metadata[page_num] = {}\n",
    "\n",
    "            for image_no, image in enumerate(images):\n",
    "                image_number = int(image_no + 1)\n",
    "                image_metadata[page_num][image_number] = {}\n",
    "\n",
    "                image_for_gemini, image_name = get_image_for_gemini(\n",
    "                    doc, image, image_no, image_save_dir, file_name, page_num\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"Extracting image from page: {page_num + 1}, saved as: {image_name}\"\n",
    "                )\n",
    "\n",
    "                response = get_gemini_response(\n",
    "                    generative_multimodal_model,\n",
    "                    model_input=[image_description_prompt, image_for_gemini],\n",
    "                    generation_config=generation_config,\n",
    "                    safety_settings=safety_settings,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "                image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
    "                    image_uri=image_name,\n",
    "                    embedding_size=embedding_size,\n",
    "                )\n",
    "\n",
    "                image_description_text_embedding = (\n",
    "                    get_text_embedding_from_text_embedding_model(text=response)\n",
    "                )\n",
    "\n",
    "                image_metadata[page_num][image_number] = {\n",
    "                    \"img_num\": image_number,\n",
    "                    \"img_path\": image_name,\n",
    "                    \"img_desc\": response,\n",
    "                    # \"mm_embedding_from_text_desc_and_img\": image_embedding_with_description,\n",
    "                    \"mm_embedding_from_img_only\": image_embedding,\n",
    "                    \"text_embedding_from_image_description\": image_description_text_embedding,\n",
    "                }\n",
    "\n",
    "            # Add sleep to reduce issues with Quota error on API\n",
    "            if add_sleep_after_page:\n",
    "                time.sleep(sleep_time_after_page)\n",
    "                print(\n",
    "                    \"Sleeping for \",\n",
    "                    sleep_time_after_page,\n",
    "                    \"\"\" sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \"\"\",\n",
    "                )\n",
    "\n",
    "        text_metadata_df = get_text_metadata_df(file_name, text_metadata)\n",
    "        image_metadata_df = get_image_metadata_df(file_name, image_metadata)\n",
    "\n",
    "        text_metadata_df_final = pd.concat(\n",
    "            [text_metadata_df_final, text_metadata_df], axis=0\n",
    "        )\n",
    "        image_metadata_df_final = pd.concat(\n",
    "            [\n",
    "                image_metadata_df_final,\n",
    "                image_metadata_df.drop_duplicates(subset=[\"img_desc\"]),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        text_metadata_df_final = text_metadata_df_final.reset_index(drop=True)\n",
    "        image_metadata_df_final = image_metadata_df_final.reset_index(drop=True)\n",
    "\n",
    "    return text_metadata_df_final, image_metadata_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8416858-e3a8-4ec0-bfee-ae3360abec57",
   "metadata": {},
   "source": [
    "다음 단계에서는 이 함수를 사용해 문서에서 텍스트와 이미지의 metadata를 추출하고 저장합니다. 다음 cell을 완료하는 데 몇 분 정도 걸릴 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a28e90-9819-4346-bb4a-96cee1a2d24a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Processing the file: --------------------------------- documents/google-10k-sample-part1.pdf \n",
      "\n",
      "\n",
      "Processing page: 1\n",
      "Processing page: 2\n",
      "Extracting image from page: 2, saved as: images/google-10k-sample-part1.pdf_image_1_0_11.jpeg\n",
      "Processing page: 3\n",
      "Extracting image from page: 3, saved as: images/google-10k-sample-part1.pdf_image_2_0_15.jpeg\n",
      "Processing page: 4\n",
      "Extracting image from page: 4, saved as: images/google-10k-sample-part1.pdf_image_3_0_18.jpeg\n",
      "Processing page: 5\n",
      "Extracting image from page: 5, saved as: images/google-10k-sample-part1.pdf_image_4_0_21.jpeg\n",
      "Processing page: 6\n",
      "Processing page: 7\n",
      "\n",
      "\n",
      " Processing the file: --------------------------------- documents/google-10k-sample-part2.pdf \n",
      "\n",
      "\n",
      "Processing page: 1\n",
      "Extracting image from page: 1, saved as: images/google-10k-sample-part2.pdf_image_0_0_6.jpeg\n",
      "Extracting image from page: 1, saved as: images/google-10k-sample-part2.pdf_image_0_1_8.jpeg\n",
      "Processing page: 2\n",
      "Extracting image from page: 2, saved as: images/google-10k-sample-part2.pdf_image_1_0_13.jpeg\n",
      "Processing page: 3\n",
      "Processing page: 4\n",
      "Extracting image from page: 4, saved as: images/google-10k-sample-part2.pdf_image_3_0_19.jpeg\n",
      "Processing page: 5\n",
      "Extracting image from page: 5, saved as: images/google-10k-sample-part2.pdf_image_4_0_22.jpeg\n",
      "Extracting image from page: 5, saved as: images/google-10k-sample-part2.pdf_image_4_1_23.jpeg\n",
      "Processing page: 6\n",
      "Extracting image from page: 6, saved as: images/google-10k-sample-part2.pdf_image_5_0_26.jpeg\n"
     ]
    }
   ],
   "source": [
    "# Specify the PDF folder with multiple PDF\n",
    "\n",
    "# pdf_folder_path = \"/content/data/\" # if running in Google Colab/Colab Enterprise\n",
    "pdf_folder_path = \"documents/\"  # if running in Vertex AI Workbench.\n",
    "\n",
    "# Specify the image description prompt. Change it\n",
    "# image_description_prompt = \"\"\"Explain what is going on in the image.\n",
    "# If it's a table, extract all elements of the table.\n",
    "# If it's a graph, explain the findings in the graph.\n",
    "# Do not include any numbers that are not mentioned in the image.\n",
    "# \"\"\"\n",
    "image_description_prompt = \"\"\"Explain what is going on in the image.\n",
    "If it's a table, extract all elements of the table.\n",
    "If it's a graph, explain the findings in the graph.\n",
    "Do not include any numbers that are not mentioned in the image.\n",
    "\"\"\"\n",
    "\n",
    "# Extract text and image metadata from the PDF document\n",
    "text_metadata_df, image_metadata_df = get_document_metadata(\n",
    "    multimodal_model,  # we are passing gemini 1.0 pro vision model\n",
    "    pdf_folder_path,\n",
    "    image_save_dir=\"images\",\n",
    "    image_description_prompt=image_description_prompt,\n",
    "    embedding_size=1408,\n",
    "    # add_sleep_after_page = True, # Uncomment this if you are running into API quota issues\n",
    "    # sleep_time_after_page = 5,\n",
    "    # generation_config = # see next cell\n",
    "    # safety_settings =  # see next cell\n",
    ")\n",
    "\n",
    "print(\"\\n\\n --- Completed processing. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728e774-f097-46ed-8911-6feeb77099fc",
   "metadata": {},
   "source": [
    "#### Inspect the processed text metadata\n",
    "\n",
    "텍스트 metadata는 아래와 같은 항목을 포함하고 있습니다.\n",
    "\n",
    "- text: 페이지의 원본 텍스트\n",
    "- text_embedding_page: 페이지의 원본 텍스트의 임베딩 벡터\n",
    "- chunk_text: 더 작은 청크로 나눈 원본 텍스트\n",
    "- chunk_number: 각 텍스트 청크의 index\n",
    "- text_embedding_chunk: 각 텍스트 청크의 임베딩 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d63d23-aee7-426b-880f-2ad477b5de1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135651f-f9dc-4bcd-918c-de1677ebedea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inspect the processed image metadata\n",
    "\n",
    "이미지 metadata는 아래와 같은 항목을 포함하고 있습니다.\n",
    "\n",
    "- img_desc: Gemini가 생성한 이미지에 대한 설명\n",
    "- mm_embedding_from_img_only: 설명 기반 분석과 비교하기 위해, 설명을 제외한 이미지만의 임베딩 벡터\n",
    "- text_embedding_from_image_description: 텍스트 분석 및 비교를 가능하도록 하기 위해, 생성된 설명의 텍스트 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d985f8-884a-43be-8255-7b91257a4788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3824b3-cb8c-41cd-954c-b41c736521ab",
   "metadata": {},
   "source": [
    "### Implement RAG\n",
    "\n",
    "위에서 추출한 document의 text/image metadata를 이용해 RAG를 구현할 차례입니다.\n",
    "\n",
    "- `get_similar_text_from_query()`: text query가 주어지면, cosine similarity 알고리즘을 사용하여 document에서 관련성이 높은 top N개의 텍스트를 검색. metadata의 text embedding을 사용하여 계산하며, 결과는 top score, page/chunk number, embedding size로 필터링 할 수 있음\n",
    "- `print_text_to_text_citation()`: get_similar_text_from_query() 함수에서 검색된 텍스트의 출처(인용문)와 세부 정보를 print\n",
    "- `get_similar_image_from_query()`: image가 주어지면, metadata의 image emgedding을 사용하여 document에서 관련성이 높은 top N개의 이미지를 검색\n",
    "- `print_text_to_image_citation()`: get_similar_image_from_query() 함수에서 검색된 이미지의 출처(인용)와 세부 정보를 print\n",
    "- `get_gemini_response()`: Gemini 모델을 이용하여 text와 image 조합을 기반으로 질문에 대한 답을 응답\n",
    "- `display_images()`: paths 또는 PIL Image objects로 제공된 이미지들을 연속하여 print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5d5ec-bece-49d1-8443-98725e3b3e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_similar_text_from_query(\n",
    "    query: str,\n",
    "    text_metadata_df: pd.DataFrame,\n",
    "    column_name: str = \"\",\n",
    "    top_n: int = 3,\n",
    "    chunk_text: bool = True,\n",
    "    print_citation: bool = False,\n",
    ") -> Dict[int, Dict[str, Any]]:\n",
    "\n",
    "    if column_name not in text_metadata_df.columns:\n",
    "        raise KeyError(f\"Column '{column_name}' not found in the 'text_metadata_df'\")\n",
    "\n",
    "    query_vector = get_user_query_text_embeddings(query)\n",
    "\n",
    "    # Calculate cosine similarity between query text and metadata text\n",
    "    cosine_scores = text_metadata_df.apply(\n",
    "        lambda row: get_cosine_score(\n",
    "            row,\n",
    "            column_name,\n",
    "            query_vector,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Get top N cosine scores and their indices\n",
    "    top_n_indices = cosine_scores.nlargest(top_n).index.tolist()\n",
    "    top_n_scores = cosine_scores.nlargest(top_n).values.tolist()\n",
    "\n",
    "    # Create a dictionary to store matched text and their information\n",
    "    final_text: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "    for matched_textno, index in enumerate(top_n_indices):\n",
    "        # Create a sub-dictionary for each matched text\n",
    "        final_text[matched_textno] = {}\n",
    "\n",
    "        # Store page number\n",
    "        final_text[matched_textno][\"file_name\"] = text_metadata_df.iloc[index][\n",
    "            \"file_name\"\n",
    "        ]\n",
    "\n",
    "        # Store page number\n",
    "        final_text[matched_textno][\"page_num\"] = text_metadata_df.iloc[index][\n",
    "            \"page_num\"\n",
    "        ]\n",
    "\n",
    "        # Store cosine score\n",
    "        final_text[matched_textno][\"cosine_score\"] = top_n_scores[matched_textno]\n",
    "\n",
    "        if chunk_text:\n",
    "            # Store chunk number\n",
    "            final_text[matched_textno][\"chunk_number\"] = text_metadata_df.iloc[index][\n",
    "                \"chunk_number\"\n",
    "            ]\n",
    "\n",
    "            # Store chunk text\n",
    "            final_text[matched_textno][\"chunk_text\"] = text_metadata_df[\"chunk_text\"][\n",
    "                index\n",
    "            ]\n",
    "        else:\n",
    "            # Store page text\n",
    "            final_text[matched_textno][\"text\"] = text_metadata_df[\"text\"][index]\n",
    "\n",
    "    # Optionally print citations immediately\n",
    "    if print_citation:\n",
    "        print_text_to_text_citation(final_text, chunk_text=chunk_text)\n",
    "\n",
    "    return final_text\n",
    "\n",
    "def print_text_to_text_citation(\n",
    "    final_text: Dict[int, Dict[str, Any]],\n",
    "    print_top: bool = True,\n",
    "    chunk_text: bool = True,\n",
    ") -> None:\n",
    "\n",
    "    color = Color()\n",
    "\n",
    "    # Iterate through the matched text citations\n",
    "    for textno, text_dict in final_text.items():\n",
    "        # Print the citation header\n",
    "        print(color.RED + f\"Citation {textno + 1}:\", \"Matched text: \\n\" + color.END)\n",
    "\n",
    "        # Print the cosine similarity score\n",
    "        print(color.BLUE + \"score: \" + color.END, text_dict[\"cosine_score\"])\n",
    "\n",
    "        # Print the file_name\n",
    "        print(color.BLUE + \"file_name: \" + color.END, text_dict[\"file_name\"])\n",
    "\n",
    "        # Print the page number\n",
    "        print(color.BLUE + \"page_number: \" + color.END, text_dict[\"page_num\"])\n",
    "\n",
    "        # Print the matched text based on the chunk_text argument\n",
    "        if chunk_text:\n",
    "            # Print chunk number and chunk text\n",
    "            print(color.BLUE + \"chunk_number: \" + color.END, text_dict[\"chunk_number\"])\n",
    "            print(color.BLUE + \"chunk_text: \" + color.END, text_dict[\"chunk_text\"])\n",
    "        else:\n",
    "            # Print page text\n",
    "            print(color.BLUE + \"page text: \" + color.END, text_dict[\"page_text\"])\n",
    "\n",
    "        # Only print the first citation if print_top is True\n",
    "        if print_top and textno == 0:\n",
    "            break\n",
    "            \n",
    "def get_similar_image_from_query(\n",
    "    text_metadata_df: pd.DataFrame,\n",
    "    image_metadata_df: pd.DataFrame,\n",
    "    query: str = \"\",\n",
    "    image_query_path: str = \"\",\n",
    "    column_name: str = \"\",\n",
    "    image_emb: bool = True,\n",
    "    top_n: int = 3,\n",
    "    embedding_size: int = 128,\n",
    ") -> Dict[int, Dict[str, Any]]:\n",
    "\n",
    "    # Check if image embedding is used\n",
    "    if image_emb:\n",
    "        # Calculate cosine similarity between query image and metadata images\n",
    "        user_query_image_embedding = get_user_query_image_embeddings(\n",
    "            image_query_path, embedding_size\n",
    "        )\n",
    "        cosine_scores = image_metadata_df.apply(\n",
    "            lambda x: get_cosine_score(x, column_name, user_query_image_embedding),\n",
    "            axis=1,\n",
    "        )\n",
    "    else:\n",
    "        # Calculate cosine similarity between query text and metadata image captions\n",
    "        user_query_text_embedding = get_user_query_text_embeddings(query)\n",
    "        cosine_scores = image_metadata_df.apply(\n",
    "            lambda x: get_cosine_score(x, column_name, user_query_text_embedding),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Remove same image comparison score when user image is matched exactly with metadata image\n",
    "    cosine_scores = cosine_scores[cosine_scores < 1.0]\n",
    "\n",
    "    # Get top N cosine scores and their indices\n",
    "    top_n_cosine_scores = cosine_scores.nlargest(top_n).index.tolist()\n",
    "    top_n_cosine_values = cosine_scores.nlargest(top_n).values.tolist()\n",
    "\n",
    "    # Create a dictionary to store matched images and their information\n",
    "    final_images: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "    for matched_imageno, indexvalue in enumerate(top_n_cosine_scores):\n",
    "        # Create a sub-dictionary for each matched image\n",
    "        final_images[matched_imageno] = {}\n",
    "\n",
    "        # Store cosine score\n",
    "        final_images[matched_imageno][\"cosine_score\"] = top_n_cosine_values[\n",
    "            matched_imageno\n",
    "        ]\n",
    "\n",
    "        # Load image from file\n",
    "        final_images[matched_imageno][\"image_object\"] = Image.load_from_file(\n",
    "            image_metadata_df.iloc[indexvalue][\"img_path\"]\n",
    "        )\n",
    "\n",
    "        # Add file name\n",
    "        final_images[matched_imageno][\"file_name\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"file_name\"\n",
    "        ]\n",
    "\n",
    "        # Store image path\n",
    "        final_images[matched_imageno][\"img_path\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"img_path\"\n",
    "        ]\n",
    "\n",
    "        # Store page number\n",
    "        final_images[matched_imageno][\"page_num\"] = image_metadata_df.iloc[indexvalue][\n",
    "            \"page_num\"\n",
    "        ]\n",
    "\n",
    "        final_images[matched_imageno][\"page_text\"] = np.unique(\n",
    "            text_metadata_df[\n",
    "                (\n",
    "                    text_metadata_df[\"page_num\"].isin(\n",
    "                        [final_images[matched_imageno][\"page_num\"]]\n",
    "                    )\n",
    "                )\n",
    "                & (\n",
    "                    text_metadata_df[\"file_name\"].isin(\n",
    "                        [final_images[matched_imageno][\"file_name\"]]\n",
    "                    )\n",
    "                )\n",
    "            ][\"text\"].values\n",
    "        )\n",
    "\n",
    "        # Store image description\n",
    "        final_images[matched_imageno][\"image_description\"] = image_metadata_df.iloc[\n",
    "            indexvalue\n",
    "        ][\"img_desc\"]\n",
    "\n",
    "    return final_images\n",
    "\n",
    "def print_text_to_image_citation(\n",
    "    final_images: Dict[int, Dict[str, Any]], print_top: bool = True\n",
    ") -> None:\n",
    "\n",
    "    color = Color()\n",
    "\n",
    "    # Iterate through the matched image citations\n",
    "    for imageno, image_dict in final_images.items():\n",
    "        # Print the citation header\n",
    "        print(\n",
    "            color.RED + f\"Citation {imageno + 1}:\",\n",
    "            \"Matched image path, page number and page text: \\n\" + color.END,\n",
    "        )\n",
    "\n",
    "        # Print the cosine similarity score\n",
    "        print(color.BLUE + \"score: \" + color.END, image_dict[\"cosine_score\"])\n",
    "\n",
    "        # Print the file_name\n",
    "        print(color.BLUE + \"file_name: \" + color.END, image_dict[\"file_name\"])\n",
    "\n",
    "        # Print the image path\n",
    "        print(color.BLUE + \"path: \" + color.END, image_dict[\"img_path\"])\n",
    "\n",
    "        # Print the page number\n",
    "        print(color.BLUE + \"page number: \" + color.END, image_dict[\"page_num\"])\n",
    "\n",
    "        # Print the page text\n",
    "        print(\n",
    "            color.BLUE + \"page text: \" + color.END, \"\\n\".join(image_dict[\"page_text\"])\n",
    "        )\n",
    "\n",
    "        # Print the image description\n",
    "        print(\n",
    "            color.BLUE + \"image description: \" + color.END,\n",
    "            image_dict[\"image_description\"],\n",
    "        )\n",
    "\n",
    "        # Only print the first citation if print_top is True\n",
    "        if print_top and imageno == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "# Add colors to the print\n",
    "class Color:\n",
    "    \"\"\"\n",
    "    This class defines a set of color codes that can be used to print text in different colors.\n",
    "    This will be used later to print citations and results to make outputs more readable.\n",
    "    \"\"\"\n",
    "\n",
    "    PURPLE: str = \"\\033[95m\"\n",
    "    CYAN: str = \"\\033[96m\"\n",
    "    DARKCYAN: str = \"\\033[36m\"\n",
    "    BLUE: str = \"\\033[94m\"\n",
    "    GREEN: str = \"\\033[92m\"\n",
    "    YELLOW: str = \"\\033[93m\"\n",
    "    RED: str = \"\\033[91m\"\n",
    "    BOLD: str = \"\\033[1m\"\n",
    "    UNDERLINE: str = \"\\033[4m\"\n",
    "    END: str = \"\\033[0m\"\n",
    "\n",
    "\n",
    "# Extracts text embeddings for the user query using a text embedding model.\n",
    "def get_user_query_text_embeddings(user_query: str) -> np.ndarray:\n",
    "    return get_text_embedding_from_text_embedding_model(user_query)\n",
    "\n",
    "# Extracts image embeddings for the user query image using a multimodal embedding model.\n",
    "def get_user_query_image_embeddings(image_query_path: str, embedding_size: int) -> np.ndarray:\n",
    "    return get_image_embedding_from_multimodal_embedding_model(image_uri=image_query_path, embedding_size=embedding_size)\n",
    "\n",
    "# Calculates the cosine similarity between the user query embedding and the dataframe embedding for a specific column.\n",
    "def get_cosine_score(dataframe: pd.DataFrame, column_name: str, input_text_embd: np.ndarray) -> float:\n",
    "    text_cosine_score = round(np.dot(dataframe[column_name], input_text_embd), 2)\n",
    "    return text_cosine_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b8de9-72b0-4e10-bbb1-7854561a48ee",
   "metadata": {},
   "source": [
    "## Text Search\n",
    "\n",
    "간단한 질문부터 시작해봅시다.\n",
    "\n",
    "- 질문: Google의 Class A, Class B, Class C 주식의 주당 기본 순이익과 희석 순이익에 대한 세부 정보를 알려주세요.\n",
    "- 예상답변: 다양한 주식 유형에 대한 Google의 주당 기본 순이익 및 희석 순이익의 가치에 대해 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa944f46-f97b-474d-a5cf-98e0d9642cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"I need details for basic and diluted net income per share of Class A, Class B, and Class C share for google?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb4919-17e7-4279-93bd-1a16ee79a957",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Search similar text with text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40c069-1ba5-4d2e-8127-81c941ab709b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Matching user text query with \"chunk_embedding\" to find relevant chunks.\n",
    "matching_results_text = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=3,\n",
    "    chunk_text=True,\n",
    ")\n",
    "\n",
    "# Print the matched text citations\n",
    "print_text_to_text_citation(matching_results_text, print_top=False, chunk_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097e198-7cb0-45c6-94bf-659cea7d576a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Search similar images with text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d18994-8b5f-49d9-9147-b0f91668dcba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_results_image = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,\n",
    "    column_name=\"text_embedding_from_image_description\",  # Use image description text embedding\n",
    "    image_emb=False,  # Use text embedding instead of image embedding\n",
    "    top_n=3,\n",
    "    embedding_size=1408,\n",
    ")\n",
    "\n",
    "# Markdown(print_text_to_image_citation(matching_results_image, print_top=True))\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "# Display the top matching image\n",
    "display(matching_results_image[0][\"image_object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e91d6-c331-42a1-b7ff-a4fed8b13dbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multimodal retrieval augmented generation (RAG)\n",
    "\n",
    "- Step 1: user query 질의\n",
    "- Step 2: text embedding을 이용하여 Document의 전체 Page에서 모든 text chunk를 검색\n",
    "- Step 3: image embedding을 이용하여 image description과 일치하는 유사한 이미지를 모두 검색\n",
    "- Step 4: Step 2,3에서 찾은 text 및 image를 context_text 및 context_images로 결합\n",
    "- Step 5: Gemini 모델에 추론 쿼리를 전달할 때, Step 2,3에서 찾은 context_text와 context_images를 함께 전달. (모델이 기억해야 할 특정 instruction을 추가할 수 있음)\n",
    "- Step 6: Gemini의 답변 확인. 답변과 함께 query를 처리하는데 사용된 관련 text 및 image를 확인할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a600e9-0198-4622-9457-56aef63546ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: User query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe10a5f-4ac1-4bd2-bbb8-30e1272bda4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the total revenues for APAC and USA for 2021?\"\n",
    "# query = \"What is deferred income taxes?\"\n",
    "# query = \"How do you compute net income per share?\"\n",
    "# query = \"What drove percentage change in the consolidated revenue and cost of revenue for the year 2021 and was there any effect of Covid?\"\n",
    "# query = \"What is the cause of 41% increase in revenue from 2020 to 2021 and how much is dollar change?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076f41c-5eb3-4e98-8c80-c0e80b75e110",
   "metadata": {},
   "source": [
    "### Step 2: Get all relevant text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92250e-b77b-4908-8beb-8ac8f4954db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve relevant chunks of text based on the query\n",
    "matching_results_chunks_data = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=10,\n",
    "    chunk_text=True,\n",
    ")\n",
    "\n",
    "# print(matching_results_chunks_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3b4de-d8f5-402a-a2d8-703953641d69",
   "metadata": {},
   "source": [
    "### Step 3: Get all relevant images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7008c93-a2de-4f03-8603-42bb255ca707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all relevant images based on user query\n",
    "matching_results_image_fromdescription_data = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,\n",
    "    column_name=\"text_embedding_from_image_description\",\n",
    "    image_emb=False,\n",
    "    top_n=10,\n",
    "    embedding_size=1408,\n",
    ")\n",
    "\n",
    "# display(matching_results_image_fromdescription_data[0][\"image_object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430e4f9-0343-4f22-98ef-c1c0ec3f5f76",
   "metadata": {},
   "source": [
    "### Step 4: Create context_text and context_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49d3b9-cf0a-4d85-b432-74b8ac1e29c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine all the selected relevant text chunks\n",
    "context_text = []\n",
    "for key, value in matching_results_chunks_data.items():\n",
    "    context_text.append(value[\"chunk_text\"])\n",
    "final_context_text = \"\\n\".join(context_text)\n",
    "\n",
    "# combine all the relevant images and their description generated by Gemini\n",
    "context_images = []\n",
    "for key, value in matching_results_image_fromdescription_data.items():\n",
    "    context_images.extend(\n",
    "        [\"Image: \", value[\"image_object\"], \"Caption: \", value[\"image_description\"]]\n",
    "    )\n",
    "\n",
    "# print(final_context_text)\n",
    "# print(context_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe88910-3740-4bfb-b2c8-ed9cf747fe53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 5: Pass context to Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c744a3-a472-4b83-97c6-cea783d99309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\" Instructions: Compare the images and the text provided as Context: to answer Question:\n",
    "Make sure to think thoroughly before answering the question and put the necessary steps to arrive at the answer in bullet points for easy explainability.\n",
    "If unsure, respond, \"Not enough context to answer\".\n",
    "\n",
    "Context:\n",
    " - Text Context:\n",
    " {final_context_text}\n",
    " - Image Context:\n",
    " {context_images}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(query)\n",
    "\n",
    "# Generate Gemini response with streaming output\n",
    "Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model,\n",
    "        model_input=[prompt],\n",
    "        stream=True,\n",
    "        generation_config=GenerationConfig(temperature=0.4, max_output_tokens=2048),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e4c2d-ca49-44b1-bf29-653af5871bb8",
   "metadata": {},
   "source": [
    "### Step 6: Print citations and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b669dc-6f2c-4665-a8ae-016f1e18afbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image citations. You can check how Gemini generated metadata helped in grounding the answer.\n",
    "print_text_to_image_citation(\n",
    "    matching_results_image_fromdescription_data, print_top=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd257222-937c-41ff-9ee0-5a82eb4274cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text citations\n",
    "print_text_to_text_citation(\n",
    "    matching_results_chunks_data,\n",
    "    print_top=False,\n",
    "    chunk_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83d35f-a43a-4da8-9930-b950c9cb4829",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "멀티모달 RAG는 매우 강력할 수 있지만 몇 가지 한계에 직면할 수 있다는 점에 유의해야 합니다.\n",
    "\n",
    "- Data dependency: 고품질의 텍스트와 시각 자료가 필요합니다.\n",
    "- Computationally demanding: 멀티모달 데이터 처리는 리소스 집약적입니다.\n",
    "- Domain specific: 일반 데이터로 학습된 모델은 의료와 같은 전문 분야에서는 빛을 발하지 못할 수 있습니다.\n",
    "- Black box: 이러한 모델의 작동 방식을 이해하는 것은 까다로워 신뢰와 채택을 방해할 수 있습니다.\n",
    "\n",
    "비록 이러한 어려움과 도전 과제들이 있지만, multimodal RAG는 다양한 데이터를 처리할 수 있는 검색 시스템을 향하기 위한 중요한 단계를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f1815-88b6-44d1-ae10-88328812a069",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reference\n",
    "\n",
    "- https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482efe6a-6f12-4a5d-8b70-8a03cb83cb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
